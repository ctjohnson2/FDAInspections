{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherjohnson/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mem():\n",
    "    alloc = torch.mps.current_allocated_memory()/10**9\n",
    "    print('Allocated:',alloc,alloc/9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_columns(df):\n",
    "    cols = []\n",
    "    for col in df.columns:\n",
    "        cols.append(col.replace(' ',''))\n",
    "    df.columns = cols\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fda_insp_df = pd.read_excel(\"inspection_details.xlsx\")\n",
    "fda_cit_df = pd.read_excel(\"citation_details.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fda_insp_df = fix_columns(fda_insp_df)\n",
    "fda_cit_df = fix_columns(fda_cit_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fda_cit_df = fda_cit_df[fda_cit_df.ProgramArea == 'Drugs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df = fda_cit_df.groupby('InspectionID')['ShortDescription'].sum().reset_index()\n",
    "long_df['ShortDescription'] = long_df['ShortDescription'].apply(lambda x: x.replace('.','. ')[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df = long_df.merge(fda_insp_df[['InspectionID','Classification','FiscalYear']].drop_duplicates()\n",
    "              ,how='left'\n",
    "              ,on='InspectionID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df = long_df[long_df.FiscalYear >=2017].drop('FiscalYear',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "insp_dups = long_df['InspectionID'].value_counts().reset_index()\n",
    "insp_dups.columns = ['InspectionID','Num']\n",
    "insp_ids = list(insp_dups[insp_dups.Num == 1]['InspectionID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_un_df = long_df[long_df['InspectionID'].isin(insp_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zh/dvg4994d0_v8dpj_6tb0y44r0000gn/T/ipykernel_92504/2799569883.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  long_un_df['Classification'] = long_un_df['Classification'].apply(lambda x: re.findall(r'\\((.*?)\\)', x)[0])\n"
     ]
    }
   ],
   "source": [
    "long_un_df['Classification'] = long_un_df['Classification'].apply(lambda x: re.findall(r'\\((.*?)\\)', x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Classification'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "long_un_df['Classification'].value_counts().plot(kind='bar',color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = 'distilbert-base-uncased'\n",
    "#model_ckpt = 'microsoft/phi-1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zh/dvg4994d0_v8dpj_6tb0y44r0000gn/T/ipykernel_92504/1474196583.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  long_un_df['ShortDescription'] = long_un_df['ShortDescription'].astype('string')\n"
     ]
    }
   ],
   "source": [
    "long_un_df['ShortDescription'] = long_un_df['ShortDescription'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zh/dvg4994d0_v8dpj_6tb0y44r0000gn/T/ipykernel_92504/3079299961.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  long_un_df['Classification'] = long_un_df['Classification'].apply(lambda x: 1 if x == 'OAI' else 0)\n"
     ]
    }
   ],
   "source": [
    "long_un_df['Classification'] = long_un_df['Classification'].apply(lambda x: 1 if x == 'OAI' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "oais_list = list(long_un_df[long_un_df.Classification == 1]['InspectionID'])\n",
    "num_oais = len(oais_list)\n",
    "non_oais_list = list(long_un_df[long_un_df.Classification == 0].sample(n = 1*num_oais, random_state=1)[\"InspectionID\"])\n",
    "long_un_sample_df = long_un_df[long_un_df['InspectionID'].isin(non_oais_list+oais_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = long_un_sample_df[['ShortDescription','Classification']]\n",
    "input_df.columns = ['text','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1068, 268)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(input_df,test_size=0.2,random_state=10)\n",
    "len(train),len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(train,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadict = DatasetDict({\"train\": Dataset.from_pandas(train), \n",
    "                        \"validation\": Dataset.from_pandas(val),\n",
    "                        \"test\": Dataset.from_pandas(test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__'],\n",
       "        num_rows: 854\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__'],\n",
       "        num_rows: 214\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__'],\n",
       "        num_rows: 268\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(ex):\n",
    "    return tokenizer(ex['text'], truncation=True,padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5619859e921d4120a6425c510aff2f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/854 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3eaa7f3c05b4a5dabe32123bc1d45d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/214 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9735be6813c04899822626d202663ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/268 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = datadict.map(tokenize_function,batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 854\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 214\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 268\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\"\n",
    "model = (AutoModelForSequenceClassification\n",
    "         .from_pretrained(model_ckpt,num_labels =2)\n",
    "         .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.267824384 0.029758264888888893\n"
     ]
    }
   ],
   "source": [
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels,preds)\n",
    "    prec = precision_score(labels,preds)\n",
    "    rec = recall_score(labels,preds)\n",
    "    return {\"Accuracy\":acc, \"F1\":f1, \"Precision\":prec, \"Recall\": rec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherjohnson/Library/Python/3.9/lib/python/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "logging_steps = len(tokenized_dataset['train']) // batch_size\n",
    "model_name = f\"{model_ckpt}-finetuned\"\n",
    "training_args = TrainingArguments(output_dir = model_name,\n",
    "                                  num_train_epochs = 5,\n",
    "                                  learning_rate = 1e-5,\n",
    "                                  per_device_train_batch_size = batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy='epoch',\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps = logging_steps,\n",
    "                                  push_to_hub=False,\n",
    "                                  log_level='error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.267824384\n",
      "1.09092864\n"
     ]
    }
   ],
   "source": [
    "print(torch.mps.current_allocated_memory()/10**9)\n",
    "print(torch.mps.driver_allocated_memory()/10**9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feef5c53c9124fff84ab8c5f9a53294e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 2.04 GB, other allocations: 6.94 GB, max allowed: 9.07 GB). Tried to allocate 89.42 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      2\u001b[0m                   args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m      3\u001b[0m                   train_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      4\u001b[0m                   eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      5\u001b[0m                   tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:3250\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3248\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3250\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/accelerate/accelerator.py:2125\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2125\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 2.04 GB, other allocations: 6.94 GB, max allowed: 9.07 GB). Tried to allocate 89.42 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  train_dataset=tokenized_dataset['train'],\n",
    "                  eval_dataset=tokenized_dataset['validation'],\n",
    "                  tokenizer=tokenizer)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 1.144068096 0.12711867733333335\n"
     ]
    }
   ],
   "source": [
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
